{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks\n",
    "\n",
    "We now use Tensorflow to classify nodes in a graph using Graph Convolution Networks.\n",
    "\n",
    "First, install gcn on your machine using the 'python setup.py install' command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise for semisupervised classification on citation networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gcn.utils import *\n",
    "from gcn.models import Model,MLP\n",
    "from gcn.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "#flags.DEFINE_float('learning_rate', 0.1, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "#flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "#flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 100, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "#--------------- Your code here --------------#\n",
    "# masked_mean_square_error(preds,labels,mask)\n",
    "\n",
    "\n",
    "#--------------------------------------------#\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "#        self.loss += masked_mean_square_error(self.outputs, self.placeholders['labels'],\n",
    "#                                                 self.placeholders['labels_mask'])\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "#    adj_square = np.power(adj,2).tocoo()\n",
    "#    return sparse_to_tuple(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.95399 train_acc= 0.07143 val_loss= 1.95070 val_acc= 0.20600 time= 0.05727\n",
      "Epoch: 0002 train_loss= 1.94801 train_acc= 0.29286 val_loss= 1.94716 val_acc= 0.37000 time= 0.01978\n",
      "Epoch: 0003 train_loss= 1.94218 train_acc= 0.48571 val_loss= 1.94333 val_acc= 0.47000 time= 0.01930\n",
      "Epoch: 0004 train_loss= 1.93654 train_acc= 0.56429 val_loss= 1.93922 val_acc= 0.50400 time= 0.02386\n",
      "Epoch: 0005 train_loss= 1.92665 train_acc= 0.66429 val_loss= 1.93517 val_acc= 0.50400 time= 0.02530\n",
      "Epoch: 0006 train_loss= 1.92017 train_acc= 0.70000 val_loss= 1.93110 val_acc= 0.51400 time= 0.02431\n",
      "Epoch: 0007 train_loss= 1.91050 train_acc= 0.71429 val_loss= 1.92704 val_acc= 0.52000 time= 0.03184\n",
      "Epoch: 0008 train_loss= 1.89941 train_acc= 0.71429 val_loss= 1.92310 val_acc= 0.51600 time= 0.02898\n",
      "Epoch: 0009 train_loss= 1.89015 train_acc= 0.75714 val_loss= 1.91920 val_acc= 0.52000 time= 0.02848\n",
      "Epoch: 0010 train_loss= 1.88369 train_acc= 0.67143 val_loss= 1.91527 val_acc= 0.52000 time= 0.03069\n",
      "Epoch: 0011 train_loss= 1.87589 train_acc= 0.72143 val_loss= 1.91122 val_acc= 0.52000 time= 0.03296\n",
      "Epoch: 0012 train_loss= 1.86910 train_acc= 0.68571 val_loss= 1.90708 val_acc= 0.53200 time= 0.02509\n",
      "Epoch: 0013 train_loss= 1.85591 train_acc= 0.70714 val_loss= 1.90297 val_acc= 0.53400 time= 0.03578\n",
      "Epoch: 0014 train_loss= 1.84738 train_acc= 0.71429 val_loss= 1.89877 val_acc= 0.55200 time= 0.03303\n",
      "Epoch: 0015 train_loss= 1.83186 train_acc= 0.72857 val_loss= 1.89443 val_acc= 0.56000 time= 0.02722\n",
      "Epoch: 0016 train_loss= 1.81894 train_acc= 0.77857 val_loss= 1.89003 val_acc= 0.57200 time= 0.03179\n",
      "Epoch: 0017 train_loss= 1.81409 train_acc= 0.73571 val_loss= 1.88559 val_acc= 0.58600 time= 0.03777\n",
      "Epoch: 0018 train_loss= 1.79738 train_acc= 0.76429 val_loss= 1.88102 val_acc= 0.59200 time= 0.03167\n",
      "Epoch: 0019 train_loss= 1.79215 train_acc= 0.75000 val_loss= 1.87632 val_acc= 0.58800 time= 0.02953\n",
      "Epoch: 0020 train_loss= 1.77766 train_acc= 0.75000 val_loss= 1.87153 val_acc= 0.59400 time= 0.03374\n",
      "Epoch: 0021 train_loss= 1.76760 train_acc= 0.77857 val_loss= 1.86662 val_acc= 0.59200 time= 0.02943\n",
      "Epoch: 0022 train_loss= 1.74438 train_acc= 0.81429 val_loss= 1.86159 val_acc= 0.58800 time= 0.03098\n",
      "Epoch: 0023 train_loss= 1.74515 train_acc= 0.78571 val_loss= 1.85646 val_acc= 0.59000 time= 0.03402\n",
      "Epoch: 0024 train_loss= 1.73049 train_acc= 0.80000 val_loss= 1.85116 val_acc= 0.59200 time= 0.03641\n",
      "Epoch: 0025 train_loss= 1.71461 train_acc= 0.77857 val_loss= 1.84557 val_acc= 0.59600 time= 0.03408\n",
      "Epoch: 0026 train_loss= 1.68822 train_acc= 0.80714 val_loss= 1.83967 val_acc= 0.60200 time= 0.03185\n",
      "Epoch: 0027 train_loss= 1.68093 train_acc= 0.78571 val_loss= 1.83355 val_acc= 0.60600 time= 0.03026\n",
      "Epoch: 0028 train_loss= 1.68646 train_acc= 0.76429 val_loss= 1.82723 val_acc= 0.62200 time= 0.03200\n",
      "Epoch: 0029 train_loss= 1.66409 train_acc= 0.79286 val_loss= 1.82073 val_acc= 0.62800 time= 0.03118\n",
      "Epoch: 0030 train_loss= 1.64123 train_acc= 0.77143 val_loss= 1.81413 val_acc= 0.63800 time= 0.03547\n",
      "Epoch: 0031 train_loss= 1.62944 train_acc= 0.80000 val_loss= 1.80741 val_acc= 0.64400 time= 0.03337\n",
      "Epoch: 0032 train_loss= 1.62115 train_acc= 0.79286 val_loss= 1.80045 val_acc= 0.65200 time= 0.03806\n",
      "Epoch: 0033 train_loss= 1.60770 train_acc= 0.82857 val_loss= 1.79344 val_acc= 0.65400 time= 0.02836\n",
      "Epoch: 0034 train_loss= 1.60463 train_acc= 0.81429 val_loss= 1.78648 val_acc= 0.66000 time= 0.03064\n",
      "Epoch: 0035 train_loss= 1.56832 train_acc= 0.85714 val_loss= 1.77930 val_acc= 0.66400 time= 0.02912\n",
      "Epoch: 0036 train_loss= 1.55106 train_acc= 0.87143 val_loss= 1.77186 val_acc= 0.67000 time= 0.02963\n",
      "Epoch: 0037 train_loss= 1.53871 train_acc= 0.87857 val_loss= 1.76430 val_acc= 0.67400 time= 0.03189\n",
      "Epoch: 0038 train_loss= 1.51639 train_acc= 0.84286 val_loss= 1.75637 val_acc= 0.68200 time= 0.03488\n",
      "Epoch: 0039 train_loss= 1.52742 train_acc= 0.87143 val_loss= 1.74819 val_acc= 0.69000 time= 0.03061\n",
      "Epoch: 0040 train_loss= 1.49984 train_acc= 0.87143 val_loss= 1.74004 val_acc= 0.68800 time= 0.03391\n",
      "Epoch: 0041 train_loss= 1.49882 train_acc= 0.85000 val_loss= 1.73166 val_acc= 0.69200 time= 0.03044\n",
      "Epoch: 0042 train_loss= 1.45390 train_acc= 0.85714 val_loss= 1.72328 val_acc= 0.69600 time= 0.03139\n",
      "Epoch: 0043 train_loss= 1.47726 train_acc= 0.83571 val_loss= 1.71492 val_acc= 0.69600 time= 0.03608\n",
      "Epoch: 0044 train_loss= 1.43801 train_acc= 0.82857 val_loss= 1.70627 val_acc= 0.70800 time= 0.03384\n",
      "Epoch: 0045 train_loss= 1.42582 train_acc= 0.84286 val_loss= 1.69771 val_acc= 0.71200 time= 0.02769\n",
      "Epoch: 0046 train_loss= 1.37447 train_acc= 0.87857 val_loss= 1.68892 val_acc= 0.72000 time= 0.02388\n",
      "Epoch: 0047 train_loss= 1.37587 train_acc= 0.82857 val_loss= 1.67985 val_acc= 0.72400 time= 0.02270\n",
      "Epoch: 0048 train_loss= 1.37041 train_acc= 0.91429 val_loss= 1.67064 val_acc= 0.73000 time= 0.02866\n",
      "Epoch: 0049 train_loss= 1.37453 train_acc= 0.84286 val_loss= 1.66152 val_acc= 0.73400 time= 0.02326\n",
      "Epoch: 0050 train_loss= 1.32568 train_acc= 0.85714 val_loss= 1.65225 val_acc= 0.73200 time= 0.02343\n",
      "Epoch: 0051 train_loss= 1.38851 train_acc= 0.85714 val_loss= 1.64305 val_acc= 0.73600 time= 0.03054\n",
      "Epoch: 0052 train_loss= 1.32950 train_acc= 0.89286 val_loss= 1.63386 val_acc= 0.73600 time= 0.03364\n",
      "Epoch: 0053 train_loss= 1.32623 train_acc= 0.89286 val_loss= 1.62455 val_acc= 0.73600 time= 0.03200\n",
      "Epoch: 0054 train_loss= 1.30045 train_acc= 0.87143 val_loss= 1.61519 val_acc= 0.74200 time= 0.03403\n",
      "Epoch: 0055 train_loss= 1.28332 train_acc= 0.89286 val_loss= 1.60586 val_acc= 0.74200 time= 0.03082\n",
      "Epoch: 0056 train_loss= 1.23221 train_acc= 0.90714 val_loss= 1.59672 val_acc= 0.74400 time= 0.03442\n",
      "Epoch: 0057 train_loss= 1.26357 train_acc= 0.90714 val_loss= 1.58782 val_acc= 0.74600 time= 0.03361\n",
      "Epoch: 0058 train_loss= 1.22354 train_acc= 0.90000 val_loss= 1.57871 val_acc= 0.74800 time= 0.03089\n",
      "Epoch: 0059 train_loss= 1.23061 train_acc= 0.91429 val_loss= 1.56945 val_acc= 0.75200 time= 0.03592\n",
      "Epoch: 0060 train_loss= 1.25688 train_acc= 0.90000 val_loss= 1.56006 val_acc= 0.75600 time= 0.03533\n",
      "Epoch: 0061 train_loss= 1.18911 train_acc= 0.89286 val_loss= 1.55086 val_acc= 0.75600 time= 0.03158\n",
      "Epoch: 0062 train_loss= 1.20369 train_acc= 0.90000 val_loss= 1.54161 val_acc= 0.75800 time= 0.03591\n",
      "Epoch: 0063 train_loss= 1.13703 train_acc= 0.89286 val_loss= 1.53249 val_acc= 0.76200 time= 0.03163\n",
      "Epoch: 0064 train_loss= 1.18960 train_acc= 0.86429 val_loss= 1.52341 val_acc= 0.76200 time= 0.02701\n",
      "Epoch: 0065 train_loss= 1.17308 train_acc= 0.92857 val_loss= 1.51439 val_acc= 0.76400 time= 0.02480\n",
      "Epoch: 0066 train_loss= 1.12569 train_acc= 0.92857 val_loss= 1.50526 val_acc= 0.77000 time= 0.02181\n",
      "Epoch: 0067 train_loss= 1.12062 train_acc= 0.88571 val_loss= 1.49642 val_acc= 0.77400 time= 0.02606\n",
      "Epoch: 0068 train_loss= 1.11809 train_acc= 0.93571 val_loss= 1.48789 val_acc= 0.77200 time= 0.02515\n",
      "Epoch: 0069 train_loss= 1.12220 train_acc= 0.91429 val_loss= 1.47969 val_acc= 0.77200 time= 0.02566\n",
      "Epoch: 0070 train_loss= 1.09568 train_acc= 0.92143 val_loss= 1.47169 val_acc= 0.77000 time= 0.02693\n",
      "Epoch: 0071 train_loss= 1.09616 train_acc= 0.88571 val_loss= 1.46357 val_acc= 0.77400 time= 0.03020\n",
      "Epoch: 0072 train_loss= 1.07255 train_acc= 0.93571 val_loss= 1.45562 val_acc= 0.77600 time= 0.02733\n",
      "Epoch: 0073 train_loss= 1.06721 train_acc= 0.96429 val_loss= 1.44818 val_acc= 0.77400 time= 0.02542\n",
      "Epoch: 0074 train_loss= 1.10403 train_acc= 0.89286 val_loss= 1.44058 val_acc= 0.77400 time= 0.02946\n",
      "Epoch: 0075 train_loss= 1.01538 train_acc= 0.92143 val_loss= 1.43314 val_acc= 0.77600 time= 0.03307\n",
      "Epoch: 0076 train_loss= 1.00960 train_acc= 0.94286 val_loss= 1.42541 val_acc= 0.77600 time= 0.03076\n",
      "Epoch: 0077 train_loss= 0.95229 train_acc= 0.95000 val_loss= 1.41774 val_acc= 0.77600 time= 0.03524\n",
      "Epoch: 0078 train_loss= 1.04013 train_acc= 0.91429 val_loss= 1.41061 val_acc= 0.77600 time= 0.04139\n",
      "Epoch: 0079 train_loss= 1.00806 train_acc= 0.92857 val_loss= 1.40379 val_acc= 0.77600 time= 0.03464\n",
      "Epoch: 0080 train_loss= 1.00069 train_acc= 0.95000 val_loss= 1.39687 val_acc= 0.77600 time= 0.03909\n",
      "Epoch: 0081 train_loss= 0.97463 train_acc= 0.91429 val_loss= 1.39013 val_acc= 0.77600 time= 0.03191\n",
      "Epoch: 0082 train_loss= 0.96301 train_acc= 0.95714 val_loss= 1.38356 val_acc= 0.77600 time= 0.02669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0083 train_loss= 0.95958 train_acc= 0.95000 val_loss= 1.37702 val_acc= 0.77600 time= 0.02707\n",
      "Epoch: 0084 train_loss= 0.97093 train_acc= 0.93571 val_loss= 1.37043 val_acc= 0.77600 time= 0.03156\n",
      "Epoch: 0085 train_loss= 0.94742 train_acc= 0.96429 val_loss= 1.36384 val_acc= 0.77600 time= 0.02068\n",
      "Epoch: 0086 train_loss= 0.94996 train_acc= 0.93571 val_loss= 1.35739 val_acc= 0.77800 time= 0.02001\n",
      "Epoch: 0087 train_loss= 0.97249 train_acc= 0.92857 val_loss= 1.35100 val_acc= 0.78000 time= 0.01884\n",
      "Epoch: 0088 train_loss= 0.94913 train_acc= 0.93571 val_loss= 1.34459 val_acc= 0.78000 time= 0.01806\n",
      "Epoch: 0089 train_loss= 0.97455 train_acc= 0.91429 val_loss= 1.33826 val_acc= 0.78000 time= 0.01990\n",
      "Epoch: 0090 train_loss= 0.97957 train_acc= 0.91429 val_loss= 1.33204 val_acc= 0.78000 time= 0.02031\n",
      "Epoch: 0091 train_loss= 0.92480 train_acc= 0.95000 val_loss= 1.32596 val_acc= 0.78000 time= 0.01888\n",
      "Epoch: 0092 train_loss= 0.95817 train_acc= 0.92143 val_loss= 1.32011 val_acc= 0.78000 time= 0.01836\n",
      "Epoch: 0093 train_loss= 0.89500 train_acc= 0.95000 val_loss= 1.31456 val_acc= 0.77800 time= 0.02101\n",
      "Epoch: 0094 train_loss= 0.86474 train_acc= 0.92857 val_loss= 1.30937 val_acc= 0.77800 time= 0.02757\n",
      "Epoch: 0095 train_loss= 0.90715 train_acc= 0.96429 val_loss= 1.30445 val_acc= 0.78000 time= 0.02758\n",
      "Epoch: 0096 train_loss= 0.89772 train_acc= 0.93571 val_loss= 1.29963 val_acc= 0.78000 time= 0.03180\n",
      "Epoch: 0097 train_loss= 0.88455 train_acc= 0.93571 val_loss= 1.29490 val_acc= 0.78000 time= 0.03080\n",
      "Epoch: 0098 train_loss= 0.93267 train_acc= 0.91429 val_loss= 1.29068 val_acc= 0.78000 time= 0.02772\n",
      "Epoch: 0099 train_loss= 0.88896 train_acc= 0.94286 val_loss= 1.28639 val_acc= 0.78400 time= 0.03198\n",
      "Epoch: 0100 train_loss= 0.90960 train_acc= 0.91429 val_loss= 1.28227 val_acc= 0.78400 time= 0.02730\n",
      "Epoch: 0101 train_loss= 0.86087 train_acc= 0.95714 val_loss= 1.27814 val_acc= 0.78400 time= 0.02492\n",
      "Epoch: 0102 train_loss= 0.86598 train_acc= 0.93571 val_loss= 1.27432 val_acc= 0.78400 time= 0.02419\n",
      "Epoch: 0103 train_loss= 0.83829 train_acc= 0.93571 val_loss= 1.27057 val_acc= 0.78400 time= 0.02498\n",
      "Epoch: 0104 train_loss= 0.84580 train_acc= 0.95000 val_loss= 1.26665 val_acc= 0.78400 time= 0.02471\n",
      "Epoch: 0105 train_loss= 0.82976 train_acc= 0.95714 val_loss= 1.26240 val_acc= 0.78000 time= 0.02457\n",
      "Epoch: 0106 train_loss= 0.86847 train_acc= 0.92857 val_loss= 1.25843 val_acc= 0.78000 time= 0.02434\n",
      "Epoch: 0107 train_loss= 0.86326 train_acc= 0.93571 val_loss= 1.25464 val_acc= 0.77800 time= 0.02446\n",
      "Epoch: 0108 train_loss= 0.83011 train_acc= 0.94286 val_loss= 1.25113 val_acc= 0.77800 time= 0.02916\n",
      "Epoch: 0109 train_loss= 0.84336 train_acc= 0.95000 val_loss= 1.24781 val_acc= 0.77800 time= 0.03142\n",
      "Epoch: 0110 train_loss= 0.78123 train_acc= 0.95000 val_loss= 1.24383 val_acc= 0.77800 time= 0.03127\n",
      "Epoch: 0111 train_loss= 0.83708 train_acc= 0.95000 val_loss= 1.24008 val_acc= 0.77800 time= 0.02232\n",
      "Epoch: 0112 train_loss= 0.85494 train_acc= 0.92857 val_loss= 1.23682 val_acc= 0.77800 time= 0.02619\n",
      "Epoch: 0113 train_loss= 0.79999 train_acc= 0.92857 val_loss= 1.23371 val_acc= 0.78000 time= 0.02506\n",
      "Epoch: 0114 train_loss= 0.79866 train_acc= 0.92857 val_loss= 1.23075 val_acc= 0.78000 time= 0.02466\n",
      "Epoch: 0115 train_loss= 0.80150 train_acc= 0.94286 val_loss= 1.22764 val_acc= 0.78000 time= 0.02957\n",
      "Epoch: 0116 train_loss= 0.81792 train_acc= 0.97143 val_loss= 1.22439 val_acc= 0.78000 time= 0.02742\n",
      "Epoch: 0117 train_loss= 0.78448 train_acc= 0.95000 val_loss= 1.22124 val_acc= 0.78400 time= 0.02493\n",
      "Epoch: 0118 train_loss= 0.79777 train_acc= 0.95000 val_loss= 1.21824 val_acc= 0.78000 time= 0.02857\n",
      "Epoch: 0119 train_loss= 0.77666 train_acc= 0.95714 val_loss= 1.21523 val_acc= 0.78200 time= 0.03278\n",
      "Epoch: 0120 train_loss= 0.77692 train_acc= 0.97143 val_loss= 1.21201 val_acc= 0.78200 time= 0.03084\n",
      "Epoch: 0121 train_loss= 0.77942 train_acc= 0.95000 val_loss= 1.20875 val_acc= 0.78400 time= 0.03281\n",
      "Epoch: 0122 train_loss= 0.78487 train_acc= 0.94286 val_loss= 1.20480 val_acc= 0.78200 time= 0.02578\n",
      "Epoch: 0123 train_loss= 0.76759 train_acc= 0.95000 val_loss= 1.20085 val_acc= 0.78200 time= 0.02551\n",
      "Epoch: 0124 train_loss= 0.74731 train_acc= 0.95000 val_loss= 1.19699 val_acc= 0.78000 time= 0.02500\n",
      "Epoch: 0125 train_loss= 0.75950 train_acc= 0.95000 val_loss= 1.19287 val_acc= 0.78000 time= 0.02538\n",
      "Epoch: 0126 train_loss= 0.75599 train_acc= 0.95714 val_loss= 1.18858 val_acc= 0.78000 time= 0.03130\n",
      "Epoch: 0127 train_loss= 0.74233 train_acc= 0.95000 val_loss= 1.18441 val_acc= 0.78000 time= 0.03316\n",
      "Epoch: 0128 train_loss= 0.75403 train_acc= 0.95714 val_loss= 1.18006 val_acc= 0.78000 time= 0.03634\n",
      "Epoch: 0129 train_loss= 0.76155 train_acc= 0.94286 val_loss= 1.17581 val_acc= 0.78000 time= 0.03349\n",
      "Epoch: 0130 train_loss= 0.74934 train_acc= 0.93571 val_loss= 1.17175 val_acc= 0.78000 time= 0.03078\n",
      "Epoch: 0131 train_loss= 0.76718 train_acc= 0.94286 val_loss= 1.16783 val_acc= 0.78200 time= 0.03146\n",
      "Epoch: 0132 train_loss= 0.74461 train_acc= 0.95000 val_loss= 1.16445 val_acc= 0.78200 time= 0.02702\n",
      "Epoch: 0133 train_loss= 0.72737 train_acc= 0.95000 val_loss= 1.16198 val_acc= 0.78200 time= 0.02540\n",
      "Epoch: 0134 train_loss= 0.72769 train_acc= 0.95000 val_loss= 1.16005 val_acc= 0.78200 time= 0.02301\n",
      "Epoch: 0135 train_loss= 0.73893 train_acc= 0.94286 val_loss= 1.15833 val_acc= 0.78200 time= 0.02397\n",
      "Epoch: 0136 train_loss= 0.70035 train_acc= 0.95714 val_loss= 1.15682 val_acc= 0.78000 time= 0.02887\n",
      "Epoch: 0137 train_loss= 0.79603 train_acc= 0.93571 val_loss= 1.15521 val_acc= 0.78200 time= 0.02410\n",
      "Epoch: 0138 train_loss= 0.71783 train_acc= 0.97143 val_loss= 1.15365 val_acc= 0.78200 time= 0.02366\n",
      "Epoch: 0139 train_loss= 0.72414 train_acc= 0.93571 val_loss= 1.15218 val_acc= 0.78200 time= 0.01933\n",
      "Epoch: 0140 train_loss= 0.76024 train_acc= 0.92857 val_loss= 1.15087 val_acc= 0.78000 time= 0.01874\n",
      "Epoch: 0141 train_loss= 0.71716 train_acc= 0.94286 val_loss= 1.15020 val_acc= 0.78400 time= 0.01846\n",
      "Epoch: 0142 train_loss= 0.65483 train_acc= 0.95000 val_loss= 1.14929 val_acc= 0.78600 time= 0.01971\n",
      "Epoch: 0143 train_loss= 0.70601 train_acc= 0.97857 val_loss= 1.14833 val_acc= 0.78600 time= 0.01969\n",
      "Epoch: 0144 train_loss= 0.75722 train_acc= 0.96429 val_loss= 1.14718 val_acc= 0.78600 time= 0.01838\n",
      "Epoch: 0145 train_loss= 0.70279 train_acc= 0.95714 val_loss= 1.14563 val_acc= 0.78600 time= 0.02277\n",
      "Epoch: 0146 train_loss= 0.68137 train_acc= 0.95714 val_loss= 1.14390 val_acc= 0.78600 time= 0.02778\n",
      "Epoch: 0147 train_loss= 0.70446 train_acc= 0.92143 val_loss= 1.14202 val_acc= 0.78600 time= 0.02642\n",
      "Epoch: 0148 train_loss= 0.67626 train_acc= 0.96429 val_loss= 1.13986 val_acc= 0.78600 time= 0.02561\n",
      "Epoch: 0149 train_loss= 0.70559 train_acc= 0.99286 val_loss= 1.13795 val_acc= 0.78600 time= 0.02066\n",
      "Epoch: 0150 train_loss= 0.65854 train_acc= 0.96429 val_loss= 1.13567 val_acc= 0.78600 time= 0.02145\n",
      "Epoch: 0151 train_loss= 0.69488 train_acc= 0.97143 val_loss= 1.13300 val_acc= 0.78400 time= 0.02381\n",
      "Epoch: 0152 train_loss= 0.68918 train_acc= 0.97857 val_loss= 1.13042 val_acc= 0.78400 time= 0.02273\n",
      "Epoch: 0153 train_loss= 0.73951 train_acc= 0.92143 val_loss= 1.12752 val_acc= 0.78400 time= 0.02619\n",
      "Epoch: 0154 train_loss= 0.65691 train_acc= 0.97857 val_loss= 1.12448 val_acc= 0.78400 time= 0.03058\n",
      "Epoch: 0155 train_loss= 0.68707 train_acc= 0.97143 val_loss= 1.12171 val_acc= 0.78600 time= 0.02696\n",
      "Epoch: 0156 train_loss= 0.70395 train_acc= 0.97143 val_loss= 1.11998 val_acc= 0.78600 time= 0.03536\n",
      "Epoch: 0157 train_loss= 0.67509 train_acc= 0.96429 val_loss= 1.11802 val_acc= 0.78600 time= 0.02697\n",
      "Epoch: 0158 train_loss= 0.70231 train_acc= 0.94286 val_loss= 1.11607 val_acc= 0.78600 time= 0.03427\n",
      "Epoch: 0159 train_loss= 0.65953 train_acc= 0.97143 val_loss= 1.11402 val_acc= 0.78400 time= 0.03032\n",
      "Epoch: 0160 train_loss= 0.70041 train_acc= 0.97143 val_loss= 1.11227 val_acc= 0.78400 time= 0.02702\n",
      "Epoch: 0161 train_loss= 0.70023 train_acc= 0.97143 val_loss= 1.11047 val_acc= 0.78200 time= 0.02510\n",
      "Epoch: 0162 train_loss= 0.68871 train_acc= 0.95714 val_loss= 1.10819 val_acc= 0.78400 time= 0.02620\n",
      "Epoch: 0163 train_loss= 0.65557 train_acc= 0.98571 val_loss= 1.10623 val_acc= 0.78200 time= 0.02228\n",
      "Epoch: 0164 train_loss= 0.73047 train_acc= 0.93571 val_loss= 1.10486 val_acc= 0.78000 time= 0.02617\n",
      "Epoch: 0165 train_loss= 0.68565 train_acc= 0.95714 val_loss= 1.10370 val_acc= 0.78000 time= 0.02639\n",
      "Epoch: 0166 train_loss= 0.65343 train_acc= 0.97143 val_loss= 1.10239 val_acc= 0.78000 time= 0.02369\n",
      "Epoch: 0167 train_loss= 0.70125 train_acc= 0.97143 val_loss= 1.10147 val_acc= 0.78200 time= 0.02810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0168 train_loss= 0.69909 train_acc= 0.95000 val_loss= 1.10069 val_acc= 0.78000 time= 0.02438\n",
      "Epoch: 0169 train_loss= 0.65012 train_acc= 0.96429 val_loss= 1.10007 val_acc= 0.78200 time= 0.02809\n",
      "Epoch: 0170 train_loss= 0.66773 train_acc= 0.95000 val_loss= 1.09972 val_acc= 0.78200 time= 0.02675\n",
      "Epoch: 0171 train_loss= 0.72109 train_acc= 0.92857 val_loss= 1.09840 val_acc= 0.78200 time= 0.02557\n",
      "Epoch: 0172 train_loss= 0.67563 train_acc= 0.95714 val_loss= 1.09644 val_acc= 0.78200 time= 0.02519\n",
      "Epoch: 0173 train_loss= 0.64704 train_acc= 0.97143 val_loss= 1.09398 val_acc= 0.78200 time= 0.02358\n",
      "Epoch: 0174 train_loss= 0.58342 train_acc= 0.98571 val_loss= 1.09138 val_acc= 0.77800 time= 0.02265\n",
      "Epoch: 0175 train_loss= 0.67515 train_acc= 0.94286 val_loss= 1.08887 val_acc= 0.77400 time= 0.02212\n",
      "Epoch: 0176 train_loss= 0.68394 train_acc= 0.93571 val_loss= 1.08582 val_acc= 0.77800 time= 0.02257\n",
      "Epoch: 0177 train_loss= 0.70085 train_acc= 0.94286 val_loss= 1.08304 val_acc= 0.77800 time= 0.02776\n",
      "Epoch: 0178 train_loss= 0.60540 train_acc= 0.98571 val_loss= 1.08044 val_acc= 0.77800 time= 0.02549\n",
      "Epoch: 0179 train_loss= 0.63127 train_acc= 0.95000 val_loss= 1.07856 val_acc= 0.78200 time= 0.02311\n",
      "Epoch: 0180 train_loss= 0.69479 train_acc= 0.93571 val_loss= 1.07661 val_acc= 0.78200 time= 0.02272\n",
      "Epoch: 0181 train_loss= 0.62900 train_acc= 0.97857 val_loss= 1.07482 val_acc= 0.78600 time= 0.02442\n",
      "Epoch: 0182 train_loss= 0.64596 train_acc= 0.92857 val_loss= 1.07334 val_acc= 0.78600 time= 0.02593\n",
      "Epoch: 0183 train_loss= 0.59649 train_acc= 0.97857 val_loss= 1.07162 val_acc= 0.78400 time= 0.02298\n",
      "Epoch: 0184 train_loss= 0.64177 train_acc= 0.96429 val_loss= 1.07015 val_acc= 0.78400 time= 0.02487\n",
      "Epoch: 0185 train_loss= 0.61356 train_acc= 0.96429 val_loss= 1.06902 val_acc= 0.78400 time= 0.02420\n",
      "Epoch: 0186 train_loss= 0.64748 train_acc= 0.95000 val_loss= 1.06780 val_acc= 0.78600 time= 0.02718\n",
      "Epoch: 0187 train_loss= 0.63458 train_acc= 0.96429 val_loss= 1.06619 val_acc= 0.78800 time= 0.03198\n",
      "Epoch: 0188 train_loss= 0.60758 train_acc= 0.97143 val_loss= 1.06476 val_acc= 0.78800 time= 0.02572\n",
      "Epoch: 0189 train_loss= 0.63303 train_acc= 0.95714 val_loss= 1.06355 val_acc= 0.78800 time= 0.02361\n",
      "Epoch: 0190 train_loss= 0.62635 train_acc= 0.98571 val_loss= 1.06231 val_acc= 0.78400 time= 0.04064\n",
      "Epoch: 0191 train_loss= 0.60898 train_acc= 0.98571 val_loss= 1.06063 val_acc= 0.78200 time= 0.02825\n",
      "Epoch: 0192 train_loss= 0.63756 train_acc= 0.95714 val_loss= 1.05901 val_acc= 0.77800 time= 0.03679\n",
      "Epoch: 0193 train_loss= 0.62371 train_acc= 0.94286 val_loss= 1.05767 val_acc= 0.77800 time= 0.03051\n",
      "Epoch: 0194 train_loss= 0.60151 train_acc= 0.96429 val_loss= 1.05636 val_acc= 0.77800 time= 0.02656\n",
      "Epoch: 0195 train_loss= 0.60843 train_acc= 0.95714 val_loss= 1.05533 val_acc= 0.77800 time= 0.02505\n",
      "Epoch: 0196 train_loss= 0.59138 train_acc= 0.97143 val_loss= 1.05411 val_acc= 0.78000 time= 0.02608\n",
      "Epoch: 0197 train_loss= 0.59821 train_acc= 0.97857 val_loss= 1.05297 val_acc= 0.78000 time= 0.02541\n",
      "Epoch: 0198 train_loss= 0.60693 train_acc= 0.97143 val_loss= 1.05188 val_acc= 0.77800 time= 0.02760\n",
      "Epoch: 0199 train_loss= 0.60899 train_acc= 0.95714 val_loss= 1.05047 val_acc= 0.77800 time= 0.02598\n",
      "Epoch: 0200 train_loss= 0.59147 train_acc= 0.97143 val_loss= 1.04964 val_acc= 0.77800 time= 0.03220\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "train_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "    \n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    validation_loss.append(cost)\n",
    "    validation_accuracy.append(acc)\n",
    "    train_loss.append(outs[1])\n",
    "    train_accuracy.append(outs[2])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and validation_loss[-1] > np.mean(validation_loss[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 1.01263 accuracy= 0.81400 time= 0.01358\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1188736a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy,label = 'train_accuracy')\n",
    "plt.plot(train_loss,label = 'train_loss')\n",
    "plt.plot(validation_accuracy, label='validation_accuracy')\n",
    "plt.plot(validation_loss,label='validaton_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. With no weight dacy / dropout  (covered in class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of training accuracy, training loss, validation accuracy and validation loss against the number of steps **without weight decay and without dropout**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Change it to $A^2$ (covered in class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of training accuracy, training loss, validation accuracy and validation loss against the number of steps **after changing the adjacency matrix to $A^2$ **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Change to the $L_2$ loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of training accuracy, training loss, validation accuracy and validation loss against the number of steps **with the loss function being $L_2$ loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------- Your code here --------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Change the input graph/label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the input graphs using a Stochastic Block Model. The number of nodes should be 500, and then feed the true labels using $20%$ of the nodes. The degree of a node must be used as the feature.\n",
    "Make a plot of training accuracy, training loss, validation accuracy and validation loss against the number of steps for this input graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------- Your code here --------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
